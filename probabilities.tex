% !TeX spellcheck = da_DK
\documentclass[11pt,a4paper,oneside]{article}

% Overordnet opsætning
\setlength\parindent{24pt}
\setlength\parskip{3pt}
\setlength{\headheight}{14pt}
\renewcommand{\baselinestretch}{1.7}

\usepackage[utf8]{inputenc}
\usepackage[danish,english]{babel}
\usepackage{amsfonts,amsmath} %math: align og gather mm.; fonts: flere forskellige symboler
\usepackage[left=2cm, right=2cm, bottom=2cm]{geometry}
\usepackage{enumitem} %anvendes til Ã¦ndring i itemize mm.
\usepackage{fancyhdr} %Opsaening af sidehoved og -fod
\usepackage{lastpage} % Side "x af X".
\usepackage[ocgcolorlinks,linkcolor=black]{hyperref}
\usepackage{indentfirst} %"Indent" efter section og chapters etc.
\usepackage{graphicx} % for plotting graphs in matrix
\usepackage{subcaption} % figure "footnotes"
\usepackage[font=small,compatibility=false]{caption}
\usepackage[nottoc,numbib]{tocbibind}    
\usepackage{ mathrsfs }
\usepackage{bbold}
\usepackage{setspace}



% bibliography
\usepackage{csquotes}
\usepackage{biblatex}
\addbibresource{bibliography.bib}

% Ændring af titler
\usepackage{sectsty}
\subsectionfont{\normalfont\bfseries}
\subsubsectionfont{\itshape}

%Rstudio pakker
\usepackage[svgnames]{xcolor}
\usepackage{listings}

\lstset{language=R,
basicstyle=\scriptsize\ttfamily,
commentstyle=\ttfamily\color{gray},
numbers=left,
numberstyle=\ttfamily\color{gray}\footnotesize,
stepnumber=1,
numbersep=5pt,
backgroundcolor=\color{white},
showspaces=false,
showstringspaces=false,
showtabs=false,
frame=single,
tabsize=2,
captionpos=b,
breaklines=true,
breakatwhitespace=false,
title=\lstname,
escapeinside={},
keywordstyle={},
morekeywords={}
}

% fancy table
\usepackage{booktabs}
\usepackage{multirow}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\rightmark}
\lfoot{\author}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

%symbolforkortelser
\newcommand{\lll}{\mathcal{L}}
\newcommand{\LL}{\Leftrightarrow}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\rb}{\right]}
\newcommand{\lb}{\left[}
\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\cc}{\mathbb{C}}
\newcommand{\aaa}{\mathbf{A}}
\newcommand{\ee}{\mathbf{E}}
\newcommand{\ff}{\mathcal{F}}
\newcommand{\ggg}{\mathcal{G}}
\newcommand{\hh}{\mathcal{H}}
\newcommand{\pp}{\mathcal{P}}
\newcommand{\qq}{\mathcal{Q}}
\newcommand{\vv}{\mathbf{V}}
\newcommand{\rr}{\mathbf{R}}
\newcommand{\nn}{\mathbf{N}}
\newcommand{\zz}{\mathbf{Z}}
\newcommand{\yy}{\mathbf{Y}}
\newcommand{\nnn}{\mathcal{N}}
\newcommand{\ii}{\mathbf{1}}
\newcommand{\bs}{\text{BS}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\sumt}{\sum_{t=1}^T}
\renewcommand{\theequation}{1.\arabic{equation}}
\DeclareMathOperator*{\argmin}{\arg\,\min}
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\selectlanguage{english}

\lstset{language=R}

% Environment for theorems etc.
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{proof}{Proof}
\newtheorem{mydef}{Definition}

% Colour citation links (otherwise they are luminescent green)
\hypersetup{colorlinks,linkcolor={black},citecolor={black},urlcolor={black}} 

% TOC depth
\setcounter{tocdepth}{2}

\title{Probabilities}
\author{William Gram \& Kristian Strand}
\date{\today}

\begin{document}

% -------------------- %
\pagenumbering{arabic}
\rfoot{\thepage}
\setcounter{page}{1}
% -------------------- %

\section{Filtered probabilities, predictive probabilities and z-scores}
We commence this section by discussing filtered probabilities, which are the probabilities of being in a state in period $t$, conditional on the data up until and including data observed at time $t$. We denote filtered probabilities by $\bar p$, and these probabilities are defined by:
\begin{align} \label{eq:filtered_probs}
    \bar p_{t\vert t}^\theta :=
        \begin{pmatrix}
            P\lp s_t = 1\vert x_{1:t};\theta\rp \\
            P\lp s_t = 2\vert x_{1:t};\theta\rp \\
            \dots\\
            P\lp s_t = N\vert x_{1:t};\theta\rp
        \end{pmatrix}.
\end{align}

From the filtered probabilities, it is possible to further derive the predicted probabilities. That is, the probability of being in some state $n$ in the next period, conditional of the data up until and including this period. These probabilities will be denoted $\tilde p$ and defined through the filtered probabilities as:
\begin{align}\label{eq:predictive_probs}
    \tilde p_{t+1\vert t}^\theta :=
        \begin{pmatrix}
            P\lp s_{t+1} = 1\vert x_{1:t};\theta\rp \\
            P\lp s_{t+1} = 2\vert x_{1:t};\theta\rp \\
            \dots\\
            P\lp s_{t+1} = N\vert x_{1:t};\theta\rp
        \end{pmatrix}
            =
        \mathbf{P} \bar p_{t\vert t}^\theta,
\end{align}

where $\mathbf{P}$ is the transition probabilities matrix,
\begin{align*}
    \mathbf{P} = 
        \begin{pmatrix}
            p_{11} & p_{21} & \dots  & p_{N1} \\
            p_{12} & p_{22} & \dots  & p_{N2} \\
            \vdots & \vdots & \ddots & \vdots \\
            p_{1N} & p_{2N} & \dots  & p_{NN}
        \end{pmatrix},
\end{align*}

recalling that $p_{ij} \equiv P\lp s_{t+1} = j \vert s_t = i\rp$. Finally, from the predictive probabilities we can compute the z-scores or pseudo-residuals used for model specification tests. It is in fact the z-scores which motivate computing the filtered probabilities. The quest is then to derive an expression for (1.1) that allows for simple computation of the filtered probabilities, from which the remaining computations follow.

\subsection{Filtered probabilities}
We resume attention to equation (1.1), notely any probability within the vector on the RHS. For instance, consider $P\lp s_t = 1\vert x_{1:t};\theta\rp = f\lp s_t = 1\vert x_{1:t};\theta\rp$, where the latter follows from the probability density function of a discreet random variable being a probability mass function \cite{grimmet}. Further, from the fundamental rule of probability calculus, the conditional probability can be rewritten as:
\begin{align}
    \pp\lp s_t = 1\vert x_{1:t};\theta\rp = \frac{f\lp s_t = 1, x_{1:t};\theta\rp}{ f\lp x_{1:t};\theta\rp},
\end{align}
using the definition of the probability density function referenced above. Further computations yield:
\begin{align}
    \frac{f\lp s_t = 1, x_{1:t};\theta\rp}{ f\lp x_{1:t};\theta\rp} 
        &= \frac{f\lp s_t = 1, x_t\vert x_{1:t-1};\theta\rp f\lp x_{1:t-1};\theta\rp}{ f\lp x_t\vert x_{1:t-1};\theta\rp f\lp x_{1:t-1};\theta\rp} \notag \\
        &= \frac{f\lp s_t = 1, x_t\vert x_{1:t-1};\theta\rp }{ f\lp x_t\vert x_{1:t-1};\theta\rp} \notag \\
        &= \frac{f\lp x_t\vert s_t = 1, x_{1:t-1};\theta\rp \pp\lp s_t = 1 \vert x_{1:t-1}; \theta\rp }{ f\lp x_t, s_t = 1\vert x_{1:t-1};\theta\rp + f\lp x_t, s_t = 2\vert x_{1:t-1};\theta\rp + \dots + f\lp x_t, s_t = N\vert x_{1:t-1};\theta\rp} \notag \\
        &= \frac{f\lp x_t\vert s_t = 1, x_{1:t-1};\theta\rp \pp\lp s_t = 1 \vert x_{1:t-1}; \theta\rp }{ \sum_{i=1}^N f\lp x_t\vert s_t = 1, x_{1:t-1};\theta\rp \pp\lp s_t = 1 \vert x_{1:t-1};\theta \rp}.
\end{align}

Conditional on being in state $s_t = 1$, the PDF in (1.4) is given by:
\begin{align}
    f\lp x_t \vert s_t = 1, x_{1:t-1}; \theta\rp = f\lp x_t \vert s_t = 1; \theta\rp = \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\lc - \frac{1}{2}\lp \frac{x_t - \mu_1}{\sigma_1}\rp^2\rc,
\end{align}

assuming a simple univariate model with a standard normal error term, and state-dependent mean and variance. Consequently, equation (1.3) has been rewritten to an equation in (1.5) which lends itself well to being implemented in any suitable programming language. The filtered probabilities differ from smoothed probabilities used in the EM-algorithm by the conditional information set. Filtered probabilities are probabilities of the state at time $t$, conditional only on data available up until time $t$, whereas smoothed probabilities condition on all available data, i.e. up until time $T$.

\subsection{Predicted probabilities}
Next we turn to the predicted probabilities, defined as the one-period ahead ($t+1$) probabilities of being in some state $i \in \{ 1, \dots N\}$, given current-period ($t$) data. For a two-state model, the probability of being in states 1 and 2 next period would be, respectively:
\begin{align*}
    \pp\lp s_{t+1} = 1\vert x_{1:t};\theta\rp = \pp\lp s_{t+1} = 1 \vert s_t = 1\rp \pp\lp s_t = 1 \vert x_{1:t}; \theta \rp + \pp\lp s_{t+1} = 1\vert s_t = 2\rp \pp\lp s_t = 2\vert x_{1:t};\theta\rp, \\
    \pp\lp s_{t+1} = 2\vert x_{1:t};\theta\rp = \pp\lp s_{t+1} = 2 \vert s_t = 1\rp \pp\lp s_t = 1 \vert x_{1:t}; \theta \rp + \pp\lp s_{t+1} = 2\vert s_t = 2\rp \pp\lp s_t = 2\vert x_{1:t};\theta\rp.
\end{align*}

Thus, the predicted probabilities can be expressed succinctly as:
\begin{align*}
    \begin{pmatrix}
        \pp\lp s_{t+1} = 1\vert x_{1:t};\theta\rp\\
        \pp\lp s_{t+1} = 2\vert x_{1:t};\theta\rp
    \end{pmatrix}
        =
    \begin{pmatrix}
        p_{11} & p_{21} \\
        p_{12} & p_{22}
    \end{pmatrix}
    \begin{pmatrix}
        \pp\lp s_t = 1 \vert x_{1:t};\theta \rp \\
        \pp\lp s_t = 2 \vert x_{1:t};\theta \rp
    \end{pmatrix}
    \LL
    \tilde p_{t+1\vert t}^\theta = \mathbf{P} \bar p_{t\vert t}^\theta,
\end{align*}

which is the definition in (1.2). That is, the above example extends to $N$ states.

\subsection{Z-scores}
Finally, we consider the z-score. The z-score is used for misspecification testing of the model under consideration, where more will follow in the misspecification tests. Formally, the $z$-score in a univariate model is given by:
\begin{align}
    z_{t+1} \equiv \pp \lp x_{t+1} \leq \tilde x_{t+1} \vert x_{1:t}, s_{1:t}\rp 
        &= \sum_{i=1}^N\pp\lp x_{t+1}\leq \tilde x_{t+1} \vert x_{1:t}, s_{1:t+1} \rp \pp\lp s_{t+1} = i\vert x_{1:t}, s_{1:t}\rp \\
        &= \sum_{i = 1}^N \Phi \lp \frac{x_{t+1} - \mu_i }{\sigma_i}\rp \pp\lp s_{t+1} = i\vert x_{1:t}s_{1:t}\rp.
\end{align}

\end{document}